{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11876025,"sourceType":"datasetVersion","datasetId":7463711},{"sourceId":11895485,"sourceType":"datasetVersion","datasetId":7477271}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport re\nVOCAB_SIZE= 42\nLABELS_SIZE= 18\n\nPAD = 0\n\n\n\nval_file=\"/kaggle/input/diacdataset/val (1).txt\"\nTEST_PATH = \"/kaggle/input/diacdataset/test.txt\"\ntrain_file=\"/kaggle/input/diacdataset/train_output (1).txt\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:19:16.957777Z","iopub.execute_input":"2025-05-21T16:19:16.958179Z","iopub.status.idle":"2025-05-21T16:19:16.962439Z","shell.execute_reply.started":"2025-05-21T16:19:16.958152Z","shell.execute_reply":"2025-05-21T16:19:16.961692Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!pip install git+https://github.com/pzelasko/kaldialign.git\n\nfrom kaldialign import edit_distance\n\n\ndef cer(ref, hyp):\n\n    ref, hyp, = ref.replace(' ', '').strip(), hyp.replace(' ', '').strip()\n    info = edit_distance(ref, hyp)\n    distance = info['total']\n    ref_length = float(len(ref))\n\n    data = {\n                'insertions': info['ins'],\n                'deletions': info['del'],\n                'substitutions': info['sub'],\n                'distance': distance,\n                'ref_length': ref_length,\n                'Error Rate': (distance / ref_length) * 100\n           }\n\n    return data\n\n\ndef wer(ref, hyp):\n\n    # build mapping of words to integers\n    b = set(ref.split() + hyp.split())\n    word2char = dict(zip(b, range(len(b))))\n\n    # map the words to a char array (Levenshtein packages only accepts strings)\n    w1 = [chr(word2char[w]) for w in ref.split()]\n    w2 = [chr(word2char[w]) for w in hyp.split()]\n\n    info = edit_distance(''.join(w1), ''.join(w2))\n    distance = info['total']\n    ref_length = float(len(w1))\n\n    data = {\n                'insertions': info['ins'],\n                'deletions': info['del'],\n                'substitutions': info['sub'],\n                'distance': distance,\n                'ref_length': ref_length,\n                'Error Rate': (distance / ref_length) * 100\n           }\n\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:19:18.536782Z","iopub.execute_input":"2025-05-21T16:19:18.537072Z","iopub.status.idle":"2025-05-21T16:19:30.913418Z","shell.execute_reply.started":"2025-05-21T16:19:18.537052Z","shell.execute_reply":"2025-05-21T16:19:30.912671Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/pzelasko/kaldialign.git\n  Cloning https://github.com/pzelasko/kaldialign.git to /tmp/pip-req-build-fn9iadw3\n  Running command git clone --filter=blob:none --quiet https://github.com/pzelasko/kaldialign.git /tmp/pip-req-build-fn9iadw3\n  Resolved https://github.com/pzelasko/kaldialign.git to commit 951315b57dd98ff5042ca56ab91310c2354b5662\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: kaldialign\n  Building wheel for kaldialign (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for kaldialign: filename=kaldialign-0.9.2-cp311-cp311-linux_x86_64.whl size=73563 sha256=f93e63078e5607e764607a87c977fbef9ca85b6814e17da577ad91c518f6d86c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-z4n2kho3/wheels/8f/0d/c6/fd63928972114bbb8302979971f76170993e5e689d020b6332\nSuccessfully built kaldialign\nInstalling collected packages: kaldialign\nSuccessfully installed kaldialign-0.9.2\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\nimport re\nCOMMA = u'\\u060C'\nSEMICOLON = u'\\u061B'\nQUESTION = u'\\u061F'\nHAMZA = u'\\u0621'\nALEF_MADDA = u'\\u0622'\nALEF_HAMZA_ABOVE = u'\\u0623'\nWAW_HAMZA = u'\\u0624'\nALEF_HAMZA_BELOW = u'\\u0625'\nYEH_HAMZA = u'\\u0626'\nALEF = u'\\u0627'\nBEH = u'\\u0628'\nTEH_MARBUTA = u'\\u0629'\nTEH = u'\\u062a'\nTHEH = u'\\u062b'\nJEEM = u'\\u062c'\nHAH = u'\\u062d'\nKHAH = u'\\u062e'\nDAL = u'\\u062f'\nTHAL = u'\\u0630'\nREH = u'\\u0631'\nZAIN = u'\\u0632'\nSEEN = u'\\u0633'\nSHEEN = u'\\u0634'\nSAD = u'\\u0635'\nDAD = u'\\u0636'\nTAH = u'\\u0637'\nZAH = u'\\u0638'\nAIN = u'\\u0639'\nGHAIN = u'\\u063a'\nTATWEEL = u'\\u0640'\nFEH = u'\\u0641'\nQAF = u'\\u0642'\nKAF = u'\\u0643'\nLAM = u'\\u0644'\nMEEM = u'\\u0645'\nNOON = u'\\u0646'\nHEH = u'\\u0647'\nWAW = u'\\u0648'\nALEF_MAKSURA = u'\\u0649'\nYEH = u'\\u064a'\nMADDA_ABOVE = u'\\u0653'\nHAMZA_ABOVE = u'\\u0654'\nHAMZA_BELOW = u'\\u0655'\nZERO = u'\\u0660'\nONE = u'\\u0661'\nTWO = u'\\u0662'\nTHREE = u'\\u0663'\nFOUR = u'\\u0664'\nFIVE = u'\\u0665'\nSIX = u'\\u0666'\nSEVEN = u'\\u0667'\nEIGHT = u'\\u0668'\nNINE = u'\\u0669'\nPERCENT = u'\\u066a'\nDECIMAL = u'\\u066b'\nTHOUSANDS = u'\\u066c'\nSTAR = u'\\u066d'\nMINI_ALEF = u'\\u0670'\nALEF_WASLA = u'\\u0671'\nFULL_STOP = u'\\u06d4'\nBYTE_ORDER_MARK = u'\\ufeff'\n\n# Diacritics\nFATHATAN = u'\\u064b'\nDAMMATAN = u'\\u064c'\nKASRATAN = u'\\u064d'\nFATHA = u'\\u064e'\nDAMMA = u'\\u064f'\nKASRA = u'\\u0650'\nSHADDA = u'\\u0651'\nSUKUN = u'\\u0652'\n\n#Ligatures\nLAM_ALEF = u'\\ufefb'\nLAM_ALEF_HAMZA_ABOVE = u'\\ufef7'\nLAM_ALEF_HAMZA_BELOW = u'\\ufef9'\nLAM_ALEF_MADDA_ABOVE = u'\\ufef5'\nSIMPLE_LAM_ALEF = u'\\u0644\\u0627'\nSIMPLE_LAM_ALEF_HAMZA_ABOVE = u'\\u0644\\u0623'\nSIMPLE_LAM_ALEF_HAMZA_BELOW = u'\\u0644\\u0625'\nSIMPLE_LAM_ALEF_MADDA_ABOVE = u'\\u0644\\u0622'\n\n\nHARAKAT_PAT = re.compile(u\"[\"+u\"\".join([FATHATAN, DAMMATAN, KASRATAN,\n                                        FATHA, DAMMA, KASRA, SUKUN,\n                                        SHADDA])+u\"]\")\nHAMZAT_PAT = re.compile(u\"[\"+u\"\".join([WAW_HAMZA, YEH_HAMZA])+u\"]\")\nALEFAT_PAT = re.compile(u\"[\"+u\"\".join([ALEF_MADDA, ALEF_HAMZA_ABOVE,\n                                       ALEF_HAMZA_BELOW, HAMZA_ABOVE,\n                                       HAMZA_BELOW])+u\"]\")\nLAMALEFAT_PAT = re.compile(u\"[\"+u\"\".join([LAM_ALEF,\n                                          LAM_ALEF_HAMZA_ABOVE,\n                                          LAM_ALEF_HAMZA_BELOW,\nLAM_ALEF_MADDA_ABOVE])+u\"]\")\n\ndef strip_tashkeel(text):\n    text = HARAKAT_PAT.sub('', text)\n    text = re.sub(u\"[\\u064E]\", \"\", text,  flags=re.UNICODE) # fattha\n    text = re.sub(u\"[\\u0671]\", \"\", text,  flags=re.UNICODE) # waSla\n    return text\ndef readFile(path):\n    sentences = []\n    with open(path, 'r', encoding='utf-8') as file:\n       for line in file:\n           sentences.append(line.strip())\n\n    return sentences\ndef strip_tatweel(text):\n    return re.sub(u'[%s]' % TATWEEL, '', text)\n\n# remove removing Tashkeel + removing Tatweel + non Arabic chars\ndef remove_non_arabic(text):\n    text = strip_tashkeel(text)\n    text = strip_tatweel(text)\n    return ' '.join(re.sub(u\"[^\\u0621-\\u063A\\u0641-\\u064A ]\", \" \", text,  flags=re.UNICODE).split())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:19:30.915127Z","iopub.execute_input":"2025-05-21T16:19:30.915428Z","iopub.status.idle":"2025-05-21T16:19:30.927730Z","shell.execute_reply.started":"2025-05-21T16:19:30.915405Z","shell.execute_reply":"2025-05-21T16:19:30.927181Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\nbuck2uni = {\n            \"'\": u\"\\u0621\", # hamza-on-the-line\n            \"|\": u\"\\u0622\", # madda\n            \">\": u\"\\u0623\", # hamza-on-'alif\n            \"&\": u\"\\u0624\", # hamza-on-waaw\n            \"<\": u\"\\u0625\", # hamza-under-'alif\n            \"}\": u\"\\u0626\", # hamza-on-yaa'\n            \"A\": u\"\\u0627\", # bare 'alif\n            \"b\": u\"\\u0628\", # baa'\n            \"p\": u\"\\u0629\", # taa' marbuuTa\n            \"t\": u\"\\u062A\", # taa'\n            \"v\": u\"\\u062B\", # thaa'\n            \"j\": u\"\\u062C\", # jiim\n            \"H\": u\"\\u062D\", # Haa'\n            \"x\": u\"\\u062E\", # khaa'\n            \"d\": u\"\\u062F\", # daal\n            \"*\": u\"\\u0630\", # dhaal\n            \"r\": u\"\\u0631\", # raa'\n            \"z\": u\"\\u0632\", # zaay\n            \"s\": u\"\\u0633\", # siin\n            \"$\": u\"\\u0634\", # shiin\n            \"S\": u\"\\u0635\", # Saad\n            \"D\": u\"\\u0636\", # Daad\n            \"T\": u\"\\u0637\", # Taa'\n            \"Z\": u\"\\u0638\", # Zaa' (DHaa')\n            \"E\": u\"\\u0639\", # cayn\n            \"g\": u\"\\u063A\", # ghayn\n            \"_\": u\"\\u0640\", # taTwiil\n            \"f\": u\"\\u0641\", # faa'\n            \"q\": u\"\\u0642\", # qaaf\n            \"k\": u\"\\u0643\", # kaaf\n            \"l\": u\"\\u0644\", # laam\n            \"m\": u\"\\u0645\", # miim\n            \"n\": u\"\\u0646\", # nuun\n            \"h\": u\"\\u0647\", # haa'\n            \"w\": u\"\\u0648\", # waaw\n            \"Y\": u\"\\u0649\", # 'alif maqSuura\n            \"y\": u\"\\u064A\", # yaa'\n            \"F\": u\"\\u064B\", # fatHatayn\n            \"N\": u\"\\u064C\", # Dammatayn\n            \"K\": u\"\\u064D\", # kasratayn\n            \"a\": u\"\\u064E\", # fatHa\n            \"u\": u\"\\u064F\", # Damma\n            \"i\": u\"\\u0650\", # kasra\n            \"~\": u\"\\u0651\", # shaddah\n            \"o\": u\"\\u0652\", # sukuun\n            \"`\": u\"\\u0670\", # dagger 'alif\n            \"{\": u\"\\u0671\", # waSla\n}\nuni2buck = {}\n\nfor (key, value) in buck2uni.items():\n    uni2buck[value] = key\n\n\nuni2buck[u\"\\ufefb\"] = \"lA\"\nuni2buck[u\"\\ufef7\"] = \"l>\"\nuni2buck[u\"\\ufef5\"] = \"l|\"\nuni2buck[u\"\\ufef9\"] = \"l<\"\n\ndef clean_text(text):\n    text = re.sub(u\"[\\ufeff]\", \"\", text,  flags=re.UNICODE)\n    text = remove_non_arabic(text)\n    return text\n\n\ndef transliterate_word(input_word, direction='bw2ar'):\n    output_word = ''\n\n    for char in input_word:\n\n        if direction == 'bw2ar':\n\n            output_word += buck2uni.get(char, char)\n        elif direction == 'ar2bw':\n\n            output_word += uni2buck.get(char, char)\n        else:\n            sys.stderr.write('Error: invalid direction!')\n            sys.exit()\n    return output_word\n\n\ndef transliterate_text(input_text, direction='bw2ar'):\n    output_text = ''\n    for input_word in input_text.split(' '):\n        output_text += transliterate_word(input_word, direction) + ' '\n\n    return output_text[:-1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:19:30.928601Z","iopub.execute_input":"2025-05-21T16:19:30.928886Z","iopub.status.idle":"2025-05-21T16:19:30.950377Z","shell.execute_reply.started":"2025-05-21T16:19:30.928862Z","shell.execute_reply":"2025-05-21T16:19:30.949708Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import re\n!pip install tensorflow\nimport tensorflow as tf\n\n\n# Diacritics\nFATHATAN = u'\\u064b'\nDAMMATAN = u'\\u064c'\nKASRATAN = u'\\u064d'\nFATHA = u'\\u064e'\nDAMMA = u'\\u064f'\nKASRA = u'\\u0650'\nSHADDA = u'\\u0651'\nSUKUN = u'\\u0652'\nTATWEEL = u'\\u0640'\n\nHARAKAT_PAT = re.compile(u\"[\"+u\"\".join([FATHATAN, DAMMATAN, KASRATAN,\n                                        FATHA, DAMMA, KASRA, SUKUN,\n                                        SHADDA])+u\"]\")\n\n\n\nclass TashkeelTokenizer:\n\n    def __init__(self):\n        self.letters = [' ', '$', '&', \"'\", '*', '<', '>', 'A', 'D', 'E', 'H', 'S', 'T', 'Y', 'Z',\n                        'b', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't',\n                        'v', 'w', 'x', 'y', 'z', '|', '}'\n                       ]\n        self.letters = ['<PAD>', '<BOS>', '<EOS>'] + self.letters + ['<MASK>']\n\n        self.no_tashkeel_tag = '<NT>'\n        self.tashkeel_list = ['<NT>', '<SD>', '<SDD>', '<SF>', '<SFF>', '<SK>',\n                               '<SKK>', 'F', 'K', 'N', 'a', 'i', 'o', 'u', '~']\n\n        self.tashkeel_list = ['<PAD>', '<BOS>', '<EOS>'] + self.tashkeel_list\n\n        self.tashkeel_map = {c:i for i,c in enumerate(self.tashkeel_list)}\n        self.letters_map = {c:i for i,c in enumerate(self.letters)}\n        self.inverse_tags = {\n                 '~a': '<SF>',  # shaddah and fatHa\n                 '~u': '<SD>',  # shaddah and Damma\n                 '~i': '<SK>',  # shaddah and kasra\n                 '~F': '<SFF>', # shaddah and fatHatayn\n                 '~N': '<SDD>', # shaddah and Dammatayn\n                 '~K': '<SKK>'  # shaddah and kasratayn\n        }\n        self.tags = {v:k for k,v in self.inverse_tags.items()}\n        self.shaddah_last  = ['a~', 'u~', 'i~', 'F~', 'N~', 'K~']\n        self.shaddah_first = ['~a', '~u', '~i', '~F', '~N', '~K']\n        self.tahkeel_chars = ['F','N','K','a', 'u', 'i', '~', 'o']\n\n\n    def clean_text(self, text):\n        text = re.sub(u'[%s]' % u'\\u0640', '', text)  # strip tatweel\n        text = text.replace('ٱ', 'ا')\n        return ' '.join(re.sub(u\"[^\\u0621-\\u063A\\u0640-\\u0652\\u0670\\u0671\\ufefb\\ufef7\\ufef5\\ufef9 ]\", \" \", text, flags=re.UNICODE).split())\n\n\n    def check_match(self, text_with_tashkeel, letter_n_tashkeel_pairs):\n        text_with_tashkeel = text_with_tashkeel.strip()\n        # test if the reconstructed text with tashkeel is the same as the original one\n        syn_text = self.combine_tashkeel_with_text(letter_n_tashkeel_pairs)\n        return syn_text == text_with_tashkeel or syn_text == self.unify_shaddah_position(text_with_tashkeel)\n\n\n    def unify_shaddah_position(self, text_with_tashkeel):\n        # unify the order of shaddah and the harakah to make shaddah always at the beginning\n        for i in range(len(self.shaddah_first)):\n            text_with_tashkeel = text_with_tashkeel.replace(self.shaddah_last[i], self.shaddah_first[i])\n        return text_with_tashkeel\n\n\n    def split_tashkeel_from_text(self, text_with_tashkeel, test_match=True):\n        text_with_tashkeel = self.clean_text(text_with_tashkeel)\n        text_with_tashkeel = transliterate_text(text_with_tashkeel, 'ar2bw')\n        text_with_tashkeel = text_with_tashkeel.replace('`', '')  # remove dagger 'alif\n\n        # unify the order of shaddah and the harakah to make shaddah always at the beginning\n        text_with_tashkeel = self.unify_shaddah_position(text_with_tashkeel)\n\n        # remove duplicated harakat\n        for i in range(len(self.tahkeel_chars)):\n            text_with_tashkeel = text_with_tashkeel.replace(self.tahkeel_chars[i]*2, self.tahkeel_chars[i])\n\n        letter_n_tashkeel_pairs = []\n        for i in range(len(text_with_tashkeel)):  # go over the whole text\n            # check if the first character is a normal letter and the second character is a tashkeel\n            if i < (len(text_with_tashkeel) - 1) and not text_with_tashkeel[i] in self.tashkeel_list and text_with_tashkeel[i+1] in self.tashkeel_list:\n                # IMPORTANT: check if tashkeel is Shaddah, then there might be another Tashkeel char associated with it. If so,\n                # replace both Shaddah and the Tashkeel chars with the appropriate tag\n                if text_with_tashkeel[i+1] == '~':\n                    # IMPORTANT: the following if statement depends on the concept of short circuit!!\n                    # The first condition checks if there are still more chars before it access position i+2\n                    # \"text_with_tashkeel[i+2]\" since it causes \"index out of range\" exception. Notice that\n                    # Shaddah here is put in the first position before the Harakah.\n                    if i+2 < len(text_with_tashkeel) and f'~{text_with_tashkeel[i+2]}' in self.inverse_tags:\n                        letter_n_tashkeel_pairs.append((text_with_tashkeel[i], self.inverse_tags[f'~{text_with_tashkeel[i+2]}']))\n                    else:\n                        # if it is only Shaddah, just add it to the list\n                        letter_n_tashkeel_pairs.append((text_with_tashkeel[i], '~'))\n                else:\n                    letter_n_tashkeel_pairs.append((text_with_tashkeel[i], text_with_tashkeel[i+1]))\n            # if the character at position i is a normal letter and has no Tashkeel, then add\n            # it with the tag \"self.no_tashkeel_tag\"\n            # IMPORTANT: this elif block ensures also that there is no two or more consecutive tashkeel other than shaddah\n            elif not text_with_tashkeel[i] in self.tashkeel_list:\n                letter_n_tashkeel_pairs.append((text_with_tashkeel[i], self.no_tashkeel_tag))\n\n        if test_match:\n            # test if the split is done correctly by ensuring that we can retrieve back the original text\n            assert self.check_match(text_with_tashkeel, letter_n_tashkeel_pairs)\n        return [('<BOS>', '<BOS>')] + letter_n_tashkeel_pairs + [('<EOS>', '<EOS>')]\n\n\n    def combine_tashkeel_with_text(self, letter_n_tashkeel_pairs):\n        combined_with_tashkeel = []\n        for letter, tashkeel in letter_n_tashkeel_pairs:\n            combined_with_tashkeel.append(letter)\n            if tashkeel in self.tags:\n                combined_with_tashkeel.append(self.tags[tashkeel])\n            elif tashkeel != self.no_tashkeel_tag:\n                combined_with_tashkeel.append(tashkeel)\n        text = ''.join(combined_with_tashkeel)\n        return text\n\n\n    def encode(self, text_with_tashkeel, test_match=False):\n        letter_n_tashkeel_pairs = self.split_tashkeel_from_text(text_with_tashkeel, test_match)\n        text, tashkeel = zip(*letter_n_tashkeel_pairs)\n        input_ids = [self.letters_map[c] for c in text]\n        target_ids = [self.tashkeel_map[c] for c in tashkeel]\n        input_tensor = tf.convert_to_tensor(input_ids, dtype=tf.int64)\n        target_tensor = tf.convert_to_tensor(target_ids, dtype=tf.int64)\n\n        return input_tensor, target_tensor\n\n\n    def filter_tashkeel(self, tashkeel):\n        tmp = []\n        for i, t in enumerate(tashkeel):\n            if i != 0 and t == '<BOS>':\n                t = self.no_tashkeel_tag\n            elif i != (len(tashkeel) - 1) and t == '<EOS>':\n                t = self.no_tashkeel_tag\n            tmp.append(t)\n        tashkeel = tmp\n        return tashkeel\n\n\n    def decode(self, input_ids, target_ids):\n        input_ids = input_ids.numpy().tolist()\n        target_ids = target_ids.numpy().tolist()\n        ar_texts = []\n        for j in range(len(input_ids)):\n            letters = [self.letters[i] for i in input_ids[j]]\n            tashkeel = [self.tashkeel_list[i] for i in target_ids[j]]\n\n            letters = list(filter(lambda x: x != '<BOS>' and x != '<EOS>' and x != '<PAD>', letters))\n            tashkeel = self.filter_tashkeel(tashkeel)\n            tashkeel = list(filter(lambda x: x != '<BOS>' and x != '<EOS>' and x != '<PAD>', tashkeel))\n\n            # VERY IMPORTANT NOTE: zip takes min(len(letters), len(tashkeel)) and discard the rest of letters / tashkeels\n            letter_n_tashkeel_pairs = list(zip(letters, tashkeel))\n            bw_text = self.combine_tashkeel_with_text(letter_n_tashkeel_pairs)\n            ar_text = transliterate_text(bw_text, 'bw2ar')\n            ar_texts.append(ar_text)\n        return ar_texts\n\n\n    def get_tashkeel_with_case_ending(self, text, case_ending=True):\n        text_split = self.split_tashkeel_from_text(text, test_match=False)\n        text_spaces_indecies = [i for i, el in enumerate(text_split) if el == (' ', '<NT>')]\n        new_text_split = []\n        for i, el in enumerate(text_split):\n            if not case_ending and (i+1) in text_spaces_indecies:\n                el = (el[0], '<NT>')  # no case ending\n            new_text_split.append(el)\n        letters, tashkeel = zip(*new_text_split)\n        return letters, tashkeel\n    def compute_der(self, ref, hyp, case_ending=True):\n        _, ref_tashkeel = self.get_tashkeel_with_case_ending(ref, case_ending=case_ending)\n        _, hyp_tashkeel = self.get_tashkeel_with_case_ending(hyp, case_ending=case_ending)\n        ref_tashkeel = ' '.join(ref_tashkeel)\n        hyp_tashkeel = ' '.join(hyp_tashkeel)\n        return wer(ref_tashkeel, hyp_tashkeel)\n\n    def compute_wer(self, ref, hyp, case_ending=True):\n        ref_letters, ref_tashkeel = self.get_tashkeel_with_case_ending(ref, case_ending=case_ending)\n        hyp_letters, hyp_tashkeel = self.get_tashkeel_with_case_ending(hyp, case_ending=case_ending)\n        ref_text_combined = self.combine_tashkeel_with_text(zip(ref_letters, ref_tashkeel))\n        hyp_text_combined = self.combine_tashkeel_with_text(zip(hyp_letters, hyp_tashkeel))\n        return wer(ref_text_combined, hyp_text_combined)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:19:33.618826Z","iopub.execute_input":"2025-05-21T16:19:33.619726Z","iopub.status.idle":"2025-05-21T16:19:36.731536Z","shell.execute_reply.started":"2025-05-21T16:19:33.619681Z","shell.execute_reply":"2025-05-21T16:19:36.730621Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.0rc1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\n# ========== RNN ==========\n\nclass RNN(Model):\n    def __init__(self, vocab_size, n_classes, embedding_dim=128, hidden_size=128, num_layers=1, dropout=0.3):\n        super(RNN, self).__init__()\n        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n        self.rnns = [\n            layers.Bidirectional(layers.LSTM(hidden_size, return_sequences=True, dropout=dropout))\n            for _ in range(num_layers)\n        ]\n        self.output_layer = layers.Dense(n_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        x = self.embedding(inputs)\n        for rnn in self.rnns:\n            x = rnn(x)\n        return self.output_layer(x)\n\n    def get_config(self):\n        return {\n            \"vocab_size\": self.embedding.input_dim,\n            \"n_classes\": self.output_layer.units,\n            \"embedding_dim\": self.embedding.output_dim,\n        }\n\n\n# ========== StackedBiGRU ==========\n\nclass StackedBiGRU(Model):\n    def __init__(self, vocab_size, n_classes, embedding_dim=128, hidden_size=128, num_layers=2, dropout=0.3):\n        super(StackedBiGRU, self).__init__()\n        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n        self.gru_layers = [\n            layers.Bidirectional(\n                layers.GRU(hidden_size, return_sequences=True, dropout=dropout),\n                merge_mode='concat'\n            )\n            for _ in range(num_layers)\n        ]\n        self.output_layer = layers.Dense(n_classes)\n\n    def call(self, inputs, training=False):\n        x = self.embedding(inputs)\n        for gru in self.gru_layers:\n            x = gru(x, training=training)\n        return self.output_layer(x)\n\n    def get_config(self):\n        return {\n            \"vocab_size\": self.embedding.input_dim,\n            \"n_classes\": self.output_layer.units,\n            \"embedding_dim\": self.embedding.output_dim,\n        }\n\n\n# ========== CBHG Layer ==========\n\nclass CBHG(tf.keras.layers.Layer):\n    def __init__(self, conv_bank_filters, proj_filters, highway_units, gru_units, **kwargs):\n        super(CBHG, self).__init__(**kwargs)\n        self.conv_bank_filters = conv_bank_filters\n        self.proj_filters = proj_filters\n        self.highway_units = highway_units\n        self.gru_units = gru_units\n\n        self.conv_bank = [layers.Conv1D(filters=conv_bank_filters, kernel_size=k, padding='same', activation='relu')\n                          for k in range(1, 9)]\n        self.batch_norms = [layers.BatchNormalization() for _ in self.conv_bank]\n\n        self.max_pool = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')\n\n        self.proj1 = layers.Conv1D(filters=proj_filters[0], kernel_size=3, padding='same', activation='relu')\n        self.proj2 = layers.Conv1D(filters=proj_filters[1], kernel_size=3, padding='same')\n\n        self.highway_layers = [layers.Dense(highway_units, activation='relu') for _ in range(4)]\n\n        self.bi_gru = layers.Bidirectional(layers.GRU(gru_units, return_sequences=True))\n\n    def call(self, inputs, training=False):\n        x = inputs\n        conv_outputs = []\n        for conv, bn in zip(self.conv_bank, self.batch_norms):\n            c = conv(x)\n            c = bn(c, training=training)\n            conv_outputs.append(c)\n\n        x = tf.concat(conv_outputs, axis=-1)\n        x = self.max_pool(x)\n        x = self.proj1(x)\n        x = self.proj2(x)\n\n        if x.shape[-1] == inputs.shape[-1]:\n            x += inputs\n\n        if x.shape[-1] != self.highway_units:\n            x = layers.Dense(self.highway_units)(x)\n\n        for layer in self.highway_layers:\n            x = layer(x)\n\n        x = self.bi_gru(x)\n        return x\n\n    def get_config(self):\n        config = super(CBHG, self).get_config()\n        config.update({\n            \"conv_bank_filters\": self.conv_bank_filters,\n            \"proj_filters\": self.proj_filters,\n            \"highway_units\": self.highway_units,\n            \"gru_units\": self.gru_units,\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n\n# ========== CBHG Model ==========\n\nclass CBHGModel(tf.keras.Model):\n    def __init__(self, vocab_size, label_size, embedding_dim=128, conv_bank_filters=128,\n                 proj_filters=[128, 128], highway_units=128, gru_units=128, **kwargs):\n        super(CBHGModel, self).__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.label_size = label_size\n        self.embedding_dim = embedding_dim\n        self.conv_bank_filters = conv_bank_filters\n        self.proj_filters = proj_filters\n        self.highway_units = highway_units\n        self.gru_units = gru_units\n\n        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n        self.cbhg = CBHG(conv_bank_filters, proj_filters, highway_units, gru_units)\n        self.classifier = layers.Dense(label_size)\n\n    def call(self, inputs, training=False):\n        x = self.embedding(inputs)\n        x = self.cbhg(x, training=training)\n        logits = self.classifier(x)\n        return logits\n\n    def get_config(self):\n        config = super(CBHGModel, self).get_config()\n        config.update({\n            \"vocab_size\": self.vocab_size,\n            \"label_size\": self.label_size,\n            \"embedding_dim\": self.embedding_dim,\n            \"conv_bank_filters\": self.conv_bank_filters,\n            \"proj_filters\": self.proj_filters,\n            \"highway_units\": self.highway_units,\n            \"gru_units\": self.gru_units,\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:19:40.558111Z","iopub.execute_input":"2025-05-21T16:19:40.558896Z","iopub.status.idle":"2025-05-21T16:19:40.575902Z","shell.execute_reply.started":"2025-05-21T16:19:40.558865Z","shell.execute_reply":"2025-05-21T16:19:40.575202Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n!pip install tensorflow\n!pip install keras-tuner\nimport tensorflow as tf\nimport keras_tuner as kt\nimport time\nimport numpy as np\nimport sys\nimport json\nfrom keras_tuner import RandomSearch\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport os \n# ----------------------------\n\n# 1. Préparation des données\n\n# ----------------------------\nos.makedirs('/kaggle/working/graphs', exist_ok=True)\ndef prepare_data_tf(path, pad_value=PAD, max_len=None):\n    tokenizer = TashkeelTokenizer()\n    corpus = readFile(path)\n    x, y = [], []\n\n    for sentence in corpus:\n        cleaned_sentence = tokenizer.clean_text(sentence)\n        input_ids, target_ids = tokenizer.encode(cleaned_sentence)\n        x.append(input_ids.numpy())\n        y.append(target_ids.numpy())\n\n    if not max_len:\n        max_len = max(max(len(seq) for seq in x), max(len(seq) for seq in y))\n\n    x_padded = tf.keras.preprocessing.sequence.pad_sequences(x, padding='post', maxlen=max_len)\n    y_padded = tf.keras.preprocessing.sequence.pad_sequences(y, padding='post', value=pad_value, maxlen=max_len)\n\n    return x_padded, y_padded, max_len\n\ndef data_loader(train_inputs, train_labels, val_inputs, val_labels, batch_size):\n    if train_inputs.shape[0] != train_labels.shape[0]:\n        raise ValueError(f\"Dimensions incompatibles: train_inputs.shape[0] = {train_inputs.shape[0]}, \"\n                         f\"train_labels.shape[0] = {train_labels.shape[0]}\")\n\n    train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)) \\\n        .shuffle(buffer_size=len(train_inputs)) \\\n        .batch(batch_size)\n    val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_labels)) \\\n        .batch(batch_size)\n    return train_dataset, val_dataset\n\n\n# --------------------------------------------\n\ndef evaluate_on_test(model, test_dataset, tokenizer: TashkeelTokenizer):\n    total_der = 0.0\n    total_wer = 0.0\n    num_examples = 0\n    predicted_texts = []\n    reference_texts = []\n\n    for inputs, labels in test_dataset:\n        logits = model(inputs, training=False)\n        predicted_indices = tf.argmax(logits, axis=-1)\n        real_indices = labels\n\n        predicted_batch_texts = tokenizer.decode(inputs, predicted_indices)\n        reference_batch_texts = tokenizer.decode(inputs, real_indices)\n\n        predicted_texts.extend(predicted_batch_texts)\n        reference_texts.extend(reference_batch_texts)\n        num_examples += len(predicted_batch_texts)\n\n    if num_examples > 0:\n        for ref, hyp in zip(reference_texts, predicted_texts):\n            der_score = tokenizer.compute_der(ref, hyp)\n            wer_score = tokenizer.compute_wer(ref, hyp)\n\n            \n\n            # Gestion robuste des formats retournés\n            if isinstance(der_score, dict):\n                der_value = list(der_score.values())[0]  # ou mets ici la clé exacte si tu la connais\n            else:\n                der_value = der_score\n\n            if isinstance(wer_score, dict):\n                wer_value = list(wer_score.values())[0]\n            else:\n                wer_value = wer_score\n\n            total_der += der_value\n            total_wer += wer_value\n\n        avg_der = total_der / num_examples\n        avg_wer = total_wer / num_examples\n    else:\n        avg_der = 0.0\n        avg_wer = 0.0\n\n    print(f\"\\n✅ Évaluation optimisée sur l'ensemble de test :\")\n    print(f\"DER: {avg_der:.4f}, WER: {avg_wer:.4f}\")\n    return avg_der, avg_wer\n\n\ndef plot_and_save(history, title, der=None, wer=None):\n    \n    \n    base = f'/kaggle/working/graphs/{title}'\n    suffix = \"\"\n    if der is not None and wer is not None:\n        suffix = f' | DER={der:.3f}, WER={wer:.3f}'\n\n    # Loss\n    plt.figure()\n    plt.plot(history.history['loss'],     label='train_loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.title(f'{title} — Loss{suffix}')\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n    plt.savefig(f'{base}_loss.png')\n    plt.close()\n\n    # Accuracy\n    plt.figure()\n    plt.plot(history.history['accuracy'],      label='train_acc')\n    plt.plot(history.history['val_accuracy'],  label='val_acc')\n    plt.title(f'{title} — Accuracy{suffix}')\n    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n    plt.savefig(f'{base}_acc.png')\n    plt.close()\n\n\n#main-pipeline\ntokenizer = TashkeelTokenizer()\n\nmodel_classes = {\n        \"CBHG\": CBHGModel,\n        \"BiGRU\": StackedBiGRU,\n        \"RNN\": RNN,\n    }\n\ndefault_hparams = {\n        \"embedding_dim\": 128,\n        \"hidden_size\": 128,\n        \"num_layers\": 2,\n        \"dropout\": 0.3,\n        \"learning_rate\": 1e-3\n    }\n\ndef load_datasets(train_file, val_file, test_file, batch_size=64):\n    \n    x_train, y_train, max_len = prepare_data_tf(train_file, max_len=None)\n    x_val, y_val, _ = prepare_data_tf(val_file, max_len=max_len)\n    x_test, y_test, _ = prepare_data_tf(test_file, max_len=max_len)\n\n    train_dataset, val_dataset = data_loader(x_train, y_train, x_val, y_val, batch_size)\n    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n\n    return (x_train, y_train, x_val, y_val, test_dataset, train_dataset, val_dataset, max_len)\n\n\ndef train_all_models(model_classes, train_dataset, val_dataset, test_dataset, tokenizer, default_hparams, num_epochs):\n    performances = {}\n\n    for name, model_class in model_classes.items():\n        print(f\"\\n🔧 Entraînement du modèle {name}...\")\n\n        if name == \"CBHG\":\n            model = model_class(\n                VOCAB_SIZE, LABELS_SIZE,\n                embedding_dim=default_hparams[\"embedding_dim\"],\n                conv_bank_filters=default_hparams[\"hidden_size\"],\n                proj_filters=[default_hparams[\"hidden_size\"]]*2,\n                highway_units=default_hparams[\"hidden_size\"],\n                gru_units=default_hparams[\"hidden_size\"]\n            )\n        else:\n            model = model_class(\n                VOCAB_SIZE, LABELS_SIZE,\n                default_hparams[\"embedding_dim\"],\n                default_hparams[\"hidden_size\"],\n                default_hparams[\"num_layers\"],\n                default_hparams[\"dropout\"]\n            )\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=default_hparams[\"learning_rate\"]),\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=['accuracy']\n        )\n\n        weights_path = f\"/kaggle/working/best_weights_{name}.weights.h5\"\n        checkpoint = ModelCheckpoint(weights_path, save_best_only=True, save_weights_only=True, monitor='val_loss', verbose=1)\n\n        history = model.fit(\n            train_dataset,\n            validation_data=val_dataset,\n            epochs=num_epochs,\n            callbacks=[checkpoint]\n        )\n\n        model.load_weights(weights_path)\n        der, wer = evaluate_on_test(model, test_dataset, tokenizer)\n        plot_and_save(history, f\"{name}_initial\", der=der, wer=wer)\n        performances[name] = {\"model\": model_class, \"DER\": der, \"WER\": wer}\n        print(f\"{name} : DER={der:.4f}, WER={wer:.4f}\")\n    \n    return performances\n    \ndef build_model_with_hp(hp, model_name, model_classes):\n    embedding_dim = hp.Choice(\"embedding_dim\", [64, 128, 256])\n    learning_rate = hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n    hidden_size = hp.Choice(\"hidden_size\", [64, 128, 256])\n    num_layers = hp.Int(\"num_layers\", 1, 3)\n    dropout = hp.Float(\"dropout\", 0.1, 0.5, step=0.1)\n\n    if model_name == \"CBHG\":\n        model = model_classes[model_name](\n            VOCAB_SIZE, LABELS_SIZE,\n            embedding_dim=embedding_dim,\n            conv_bank_filters=hidden_size,\n            proj_filters=[hidden_size, hidden_size],\n            highway_units=hidden_size,\n            gru_units=hidden_size\n        )\n    else:\n        model = model_classes[model_name](\n            VOCAB_SIZE, LABELS_SIZE,\n            embedding_dim,\n            hidden_size,\n            num_layers,\n            dropout\n        )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"]\n    )\n\n    return model\n   \n       \ndef tune_best_model(build_fn, model_name, train_dataset, val_dataset):\n    tuner = kt.RandomSearch(\n        build_fn,\n        objective=\"val_accuracy\",\n        max_trials=10,\n        directory=\"tuner_dir\",\n        project_name=f\"tuning_{model_name}\",\n        overwrite=True\n    )\n    \n    tuner.search(train_dataset, epochs=5, validation_data=val_dataset)\n    \n    return tuner.get_best_hyperparameters(1)[0]\n\n\ndef retrain_best_model(best_model_name, best_hp, model_classes, train_dataset, val_dataset, test_dataset, tokenizer, num_epochs):\n    if best_model_name == \"CBHG\":\n        model = model_classes[best_model_name](\n            VOCAB_SIZE, LABELS_SIZE,\n            embedding_dim=best_hp.get(\"embedding_dim\"),\n            conv_bank_filters=best_hp.get(\"hidden_size\"),\n            proj_filters=[best_hp.get(\"hidden_size\")] * 2,\n            highway_units=best_hp.get(\"hidden_size\"),\n            gru_units=best_hp.get(\"hidden_size\")\n        )\n    else:\n        model = model_classes[best_model_name](\n            VOCAB_SIZE, LABELS_SIZE,\n            best_hp.get(\"embedding_dim\"),\n            best_hp.get(\"hidden_size\"),\n            best_hp.get(\"num_layers\"),\n            best_hp.get(\"dropout\")\n        )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=best_hp.get(\"learning_rate\")),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"]\n    )\n\n    weights_path = f\"/kaggle/working/best_weights_{best_model_name}_tuned.weights.h5\"\n    checkpoint = ModelCheckpoint(weights_path, save_best_only=True, save_weights_only=True, monitor='val_loss', verbose=1)\n\n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=num_epochs,\n        callbacks=[checkpoint]\n    )\n\n    model.load_weights(weights_path)\n    der, wer = evaluate_on_test(model, test_dataset, tokenizer)\n    plot_and_save(history, f\"{best_model_name}_tuned\", der=der, wer=wer)\n\n    return model, der, wer\n\ndef save_results(model, model_name, der, wer, performances):\n    summary = {\n        \"initial\": {name: {\"DER\": perf[\"DER\"], \"WER\": perf[\"WER\"]} for name, perf in performances.items()},\n        \"tuned\": {model_name: {\"DER\": der, \"WER\": wer}}\n    }\n\n    json_path = \"/kaggle/working/metrics_summary.json\"\n    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(summary, f, ensure_ascii=False, indent=2)\n    print(f\"Métriques sauvegardées dans {json_path}\")\n\n    model_path = f\"/kaggle/working/best_model_{model_name}.keras\"\n    model.save(model_path, save_format=\"keras\")\n    print(f\"✅ Modèle complet sauvegardé dans {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:19:45.783598Z","iopub.execute_input":"2025-05-21T16:19:45.784313Z","iopub.status.idle":"2025-05-21T16:19:51.961376Z","shell.execute_reply.started":"2025-05-21T16:19:45.784291Z","shell.execute_reply":"2025-05-21T16:19:51.960470Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.0rc1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nRequirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\nRequirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\nRequirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.4.26)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras->keras-tuner) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras->keras-tuner) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->keras->keras-tuner) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->keras->keras-tuner) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->keras->keras-tuner) (2024.2.0)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"x_train, y_train, x_val, y_val, test_dataset, train_dataset, val_dataset, max_len=load_datasets(train_file, val_file, TEST_PATH,64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:20:09.813181Z","iopub.execute_input":"2025-05-21T16:20:09.813812Z","iopub.status.idle":"2025-05-21T16:21:58.112235Z","shell.execute_reply.started":"2025-05-21T16:20:09.813791Z","shell.execute_reply":"2025-05-21T16:21:58.111689Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1747844411.932457      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1747844411.933157      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"🚀 Entraînement initial de tous les modèles avec hyperparamètres par défaut...\\n\")\nperformances = train_all_models(model_classes, train_dataset, val_dataset, test_dataset, tokenizer, default_hparams,10)\n\nbest_model_name = min(performances, key=lambda k: performances[k][\"DER\"])\nprint(f\"\\nModèle sélectionné pour le tuning : {best_model_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:22:01.139151Z","iopub.execute_input":"2025-05-21T16:22:01.139713Z","iopub.status.idle":"2025-05-21T17:08:50.885739Z","shell.execute_reply.started":"2025-05-21T16:22:01.139688Z","shell.execute_reply":"2025-05-21T17:08:50.884873Z"}},"outputs":[{"name":"stdout","text":"🚀 Entraînement initial de tous les modèles avec hyperparamètres par défaut...\n\n\n🔧 Entraînement du modèle CBHG...\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1747844537.712111     523 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8603 - loss: 0.4244\nEpoch 1: val_loss improved from inf to 0.11303, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 136ms/step - accuracy: 0.8604 - loss: 0.4242 - val_accuracy: 0.9634 - val_loss: 0.1130\nEpoch 2/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9659 - loss: 0.1047\nEpoch 2: val_loss improved from 0.11303 to 0.09399, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 141ms/step - accuracy: 0.9659 - loss: 0.1047 - val_accuracy: 0.9692 - val_loss: 0.0940\nEpoch 3/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9714 - loss: 0.0876\nEpoch 3: val_loss improved from 0.09399 to 0.08690, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 142ms/step - accuracy: 0.9714 - loss: 0.0876 - val_accuracy: 0.9714 - val_loss: 0.0869\nEpoch 4/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9744 - loss: 0.0775\nEpoch 4: val_loss improved from 0.08690 to 0.07866, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 143ms/step - accuracy: 0.9744 - loss: 0.0775 - val_accuracy: 0.9742 - val_loss: 0.0787\nEpoch 5/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9764 - loss: 0.0713\nEpoch 5: val_loss improved from 0.07866 to 0.07337, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 142ms/step - accuracy: 0.9764 - loss: 0.0713 - val_accuracy: 0.9759 - val_loss: 0.0734\nEpoch 6/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9778 - loss: 0.0671\nEpoch 6: val_loss improved from 0.07337 to 0.07125, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 141ms/step - accuracy: 0.9778 - loss: 0.0671 - val_accuracy: 0.9766 - val_loss: 0.0712\nEpoch 7/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9788 - loss: 0.0637\nEpoch 7: val_loss improved from 0.07125 to 0.07004, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 142ms/step - accuracy: 0.9788 - loss: 0.0637 - val_accuracy: 0.9771 - val_loss: 0.0700\nEpoch 8/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9796 - loss: 0.0613\nEpoch 8: val_loss improved from 0.07004 to 0.06724, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 142ms/step - accuracy: 0.9796 - loss: 0.0613 - val_accuracy: 0.9780 - val_loss: 0.0672\nEpoch 9/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9803 - loss: 0.0587\nEpoch 9: val_loss improved from 0.06724 to 0.06634, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 142ms/step - accuracy: 0.9803 - loss: 0.0587 - val_accuracy: 0.9782 - val_loss: 0.0663\nEpoch 10/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9809 - loss: 0.0571\nEpoch 10: val_loss improved from 0.06634 to 0.06518, saving model to /kaggle/working/best_weights_CBHG.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 142ms/step - accuracy: 0.9809 - loss: 0.0571 - val_accuracy: 0.9785 - val_loss: 0.0652\n\n✅ Évaluation optimisée sur l'ensemble de test :\nDER: 0.3173, WER: 0.0000\nCBHG : DER=0.3173, WER=0.0000\n\n🔧 Entraînement du modèle BiGRU...\nEpoch 1/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7718 - loss: 0.6771\nEpoch 1: val_loss improved from inf to 0.19781, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 87ms/step - accuracy: 0.7719 - loss: 0.6767 - val_accuracy: 0.9335 - val_loss: 0.1978\nEpoch 2/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9390 - loss: 0.1823\nEpoch 2: val_loss improved from 0.19781 to 0.14747, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 85ms/step - accuracy: 0.9390 - loss: 0.1823 - val_accuracy: 0.9512 - val_loss: 0.1475\nEpoch 3/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9528 - loss: 0.1431\nEpoch 3: val_loss improved from 0.14747 to 0.12871, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 84ms/step - accuracy: 0.9528 - loss: 0.1431 - val_accuracy: 0.9576 - val_loss: 0.1287\nEpoch 4/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9587 - loss: 0.1260\nEpoch 4: val_loss improved from 0.12871 to 0.11836, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 84ms/step - accuracy: 0.9587 - loss: 0.1260 - val_accuracy: 0.9611 - val_loss: 0.1184\nEpoch 5/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9620 - loss: 0.1157\nEpoch 5: val_loss improved from 0.11836 to 0.11191, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 84ms/step - accuracy: 0.9620 - loss: 0.1157 - val_accuracy: 0.9634 - val_loss: 0.1119\nEpoch 6/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9641 - loss: 0.1094\nEpoch 6: val_loss improved from 0.11191 to 0.10770, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 84ms/step - accuracy: 0.9641 - loss: 0.1094 - val_accuracy: 0.9648 - val_loss: 0.1077\nEpoch 7/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9656 - loss: 0.1051\nEpoch 7: val_loss improved from 0.10770 to 0.10366, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 84ms/step - accuracy: 0.9656 - loss: 0.1051 - val_accuracy: 0.9661 - val_loss: 0.1037\nEpoch 8/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9668 - loss: 0.1011\nEpoch 8: val_loss improved from 0.10366 to 0.10076, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 84ms/step - accuracy: 0.9668 - loss: 0.1011 - val_accuracy: 0.9670 - val_loss: 0.1008\nEpoch 9/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9677 - loss: 0.0987\nEpoch 9: val_loss improved from 0.10076 to 0.09771, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 84ms/step - accuracy: 0.9677 - loss: 0.0987 - val_accuracy: 0.9680 - val_loss: 0.0977\nEpoch 10/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9685 - loss: 0.0960\nEpoch 10: val_loss improved from 0.09771 to 0.09611, saving model to /kaggle/working/best_weights_BiGRU.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 84ms/step - accuracy: 0.9685 - loss: 0.0960 - val_accuracy: 0.9684 - val_loss: 0.0961\n\n✅ Évaluation optimisée sur l'ensemble de test :\nDER: 0.4922, WER: 0.0002\nBiGRU : DER=0.4922, WER=0.0002\n\n🔧 Entraînement du modèle RNN...\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py:708: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7195 - loss: 0.8191\nEpoch 1: val_loss improved from inf to 0.24938, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 95ms/step - accuracy: 0.7196 - loss: 0.8187 - val_accuracy: 0.9155 - val_loss: 0.2494\nEpoch 2/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9230 - loss: 0.2278\nEpoch 2: val_loss improved from 0.24938 to 0.17923, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9230 - loss: 0.2278 - val_accuracy: 0.9402 - val_loss: 0.1792\nEpoch 3/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9430 - loss: 0.1713\nEpoch 3: val_loss improved from 0.17923 to 0.15051, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9430 - loss: 0.1713 - val_accuracy: 0.9502 - val_loss: 0.1505\nEpoch 4/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9516 - loss: 0.1466\nEpoch 4: val_loss improved from 0.15051 to 0.13406, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9516 - loss: 0.1466 - val_accuracy: 0.9557 - val_loss: 0.1341\nEpoch 5/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9566 - loss: 0.1314\nEpoch 5: val_loss improved from 0.13406 to 0.12368, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9566 - loss: 0.1314 - val_accuracy: 0.9589 - val_loss: 0.1237\nEpoch 6/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9602 - loss: 0.1200\nEpoch 6: val_loss improved from 0.12368 to 0.11439, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9602 - loss: 0.1200 - val_accuracy: 0.9622 - val_loss: 0.1144\nEpoch 7/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9628 - loss: 0.1122\nEpoch 7: val_loss improved from 0.11439 to 0.10933, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9628 - loss: 0.1122 - val_accuracy: 0.9639 - val_loss: 0.1093\nEpoch 8/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9648 - loss: 0.1064\nEpoch 8: val_loss improved from 0.10933 to 0.10438, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9648 - loss: 0.1064 - val_accuracy: 0.9654 - val_loss: 0.1044\nEpoch 9/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9661 - loss: 0.1024\nEpoch 9: val_loss improved from 0.10438 to 0.10149, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9661 - loss: 0.1024 - val_accuracy: 0.9666 - val_loss: 0.1015\nEpoch 10/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9675 - loss: 0.0983\nEpoch 10: val_loss improved from 0.10149 to 0.09776, saving model to /kaggle/working/best_weights_RNN.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 94ms/step - accuracy: 0.9675 - loss: 0.0983 - val_accuracy: 0.9678 - val_loss: 0.0978\n\n✅ Évaluation optimisée sur l'ensemble de test :\nDER: 0.4918, WER: 0.0002\nRNN : DER=0.4918, WER=0.0002\n\nModèle sélectionné pour le tuning : CBHG\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"build_fn = lambda hp: build_model_with_hp(hp, best_model_name, model_classes)\nbest_hp = tune_best_model(build_fn,best_model_name,train_dataset,val_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:09:07.994724Z","iopub.execute_input":"2025-05-21T17:09:07.995232Z","iopub.status.idle":"2025-05-21T19:22:29.911184Z","shell.execute_reply.started":"2025-05-21T17:09:07.995200Z","shell.execute_reply":"2025-05-21T19:22:29.910591Z"}},"outputs":[{"name":"stdout","text":"Trial 10 Complete [00h 18m 40s]\nval_accuracy: 0.33399438858032227\n\nBest val_accuracy So Far: 0.9786353707313538\nTotal elapsed time: 02h 13m 22s\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(\"\\n Réentraînement du meilleur modèle avec ses hyperparamètres optimaux...\")\nbest_model, der_final, wer_final = retrain_best_model(best_model_name, best_hp, model_classes, train_dataset, val_dataset, test_dataset, tokenizer,10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T19:23:02.991229Z","iopub.execute_input":"2025-05-21T19:23:02.991921Z","iopub.status.idle":"2025-05-21T20:08:51.399063Z","shell.execute_reply.started":"2025-05-21T19:23:02.991898Z","shell.execute_reply":"2025-05-21T20:08:51.398378Z"}},"outputs":[{"name":"stdout","text":"\n Réentraînement du meilleur modèle avec ses hyperparamètres optimaux...\nEpoch 1/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.8592 - loss: 0.4192\nEpoch 1: val_loss improved from inf to 0.09674, saving model to /kaggle/working/best_weights_CBHG_tuned.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 333ms/step - accuracy: 0.8592 - loss: 0.4190 - val_accuracy: 0.9686 - val_loss: 0.0967\nEpoch 2/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.9710 - loss: 0.0888\nEpoch 2: val_loss improved from 0.09674 to 0.08007, saving model to /kaggle/working/best_weights_CBHG_tuned.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 330ms/step - accuracy: 0.9710 - loss: 0.0888 - val_accuracy: 0.9735 - val_loss: 0.0801\nEpoch 3/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - accuracy: 0.9761 - loss: 0.0725\nEpoch 3: val_loss improved from 0.08007 to 0.07170, saving model to /kaggle/working/best_weights_CBHG_tuned.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 328ms/step - accuracy: 0.9761 - loss: 0.0725 - val_accuracy: 0.9766 - val_loss: 0.0717\nEpoch 4/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.9788 - loss: 0.0639\nEpoch 4: val_loss improved from 0.07170 to 0.06703, saving model to /kaggle/working/best_weights_CBHG_tuned.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 329ms/step - accuracy: 0.9788 - loss: 0.0639 - val_accuracy: 0.9779 - val_loss: 0.0670\nEpoch 5/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.9802 - loss: 0.0591\nEpoch 5: val_loss improved from 0.06703 to 0.06423, saving model to /kaggle/working/best_weights_CBHG_tuned.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 329ms/step - accuracy: 0.9802 - loss: 0.0591 - val_accuracy: 0.9789 - val_loss: 0.0642\nEpoch 6/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.9814 - loss: 0.0555\nEpoch 6: val_loss improved from 0.06423 to 0.06267, saving model to /kaggle/working/best_weights_CBHG_tuned.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 326ms/step - accuracy: 0.9814 - loss: 0.0555 - val_accuracy: 0.9794 - val_loss: 0.0627\nEpoch 7/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.9825 - loss: 0.0519\nEpoch 7: val_loss did not improve from 0.06267\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 326ms/step - accuracy: 0.9825 - loss: 0.0519 - val_accuracy: 0.9795 - val_loss: 0.0628\nEpoch 8/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.9833 - loss: 0.0498\nEpoch 8: val_loss improved from 0.06267 to 0.06133, saving model to /kaggle/working/best_weights_CBHG_tuned.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 325ms/step - accuracy: 0.9833 - loss: 0.0498 - val_accuracy: 0.9800 - val_loss: 0.0613\nEpoch 9/10\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.9848 - loss: 0.0450\nEpoch 10: val_loss improved from 0.05953 to 0.05950, saving model to /kaggle/working/best_weights_CBHG_tuned.weights.h5\n\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 326ms/step - accuracy: 0.9848 - loss: 0.0450 - val_accuracy: 0.9806 - val_loss: 0.0595\n\n✅ Évaluation optimisée sur l'ensemble de test :\nDER: 0.2991, WER: 0.0000\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"save_results(best_model, best_model_name, der_final, wer_final, performances)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:10:22.756913Z","iopub.execute_input":"2025-05-21T20:10:22.757593Z","iopub.status.idle":"2025-05-21T20:10:23.079691Z","shell.execute_reply.started":"2025-05-21T20:10:22.757573Z","shell.execute_reply":"2025-05-21T20:10:23.078968Z"}},"outputs":[{"name":"stdout","text":"Métriques sauvegardées dans /kaggle/working/metrics_summary.json\n✅ Modèle complet sauvegardé dans /kaggle/working/best_model_CBHG.keras\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import json\nimport os\n\ndef save_results(model, model_name, der, wer, performances):\n    # Construction du résumé des performances\n    summary = {\n        \"initial\": {\n            name: {\"DER\": perf.get(\"DER\", None), \"WER\": perf.get(\"WER\", None)}\n            for name, perf in performances.items()\n        },\n        \"tuned\": {\n            model_name: {\"DER\": der, \"WER\": wer}\n        }\n    }\n\n    # Sauvegarde des métriques\n    json_path = os.path.join(\"/kaggle/working\", \"metrics_summary.json\")\n    try:\n        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(summary, f, ensure_ascii=False, indent=2)\n        print(f\"📄 Métriques sauvegardées dans {json_path}\")\n    except Exception as e:\n        print(f\"❌ Erreur lors de la sauvegarde des métriques : {e}\")\n\n    # Sauvegarde du modèle en .h5\n    model_path = os.path.join(\"/kaggle/working\", f\"best_model_{model_name}.h5\")\n    try:\n        model.save(model_path)\n        print(f\"✅ Modèle sauvegardé dans {model_path}\")\n    except Exception as e:\n        print(f\"❌ Erreur lors de la sauvegarde du modèle : {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:10:30.038752Z","iopub.execute_input":"2025-05-21T20:10:30.039038Z","iopub.status.idle":"2025-05-21T20:10:30.044869Z","shell.execute_reply.started":"2025-05-21T20:10:30.039018Z","shell.execute_reply":"2025-05-21T20:10:30.044301Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"save_results(best_model, best_model_name, der_final, wer_final, performances)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:10:33.279443Z","iopub.execute_input":"2025-05-21T20:10:33.280013Z","iopub.status.idle":"2025-05-21T20:10:33.464985Z","shell.execute_reply.started":"2025-05-21T20:10:33.279991Z","shell.execute_reply":"2025-05-21T20:10:33.464343Z"}},"outputs":[{"name":"stdout","text":"📄 Métriques sauvegardées dans /kaggle/working/metrics_summary.json\n✅ Modèle sauvegardé dans /kaggle/working/best_model_CBHG.h5\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"best_model.save('/kaggle/working/best_model.keras')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:19:21.603385Z","iopub.execute_input":"2025-05-21T20:19:21.603651Z","iopub.status.idle":"2025-05-21T20:19:21.933592Z","shell.execute_reply.started":"2025-05-21T20:19:21.603628Z","shell.execute_reply":"2025-05-21T20:19:21.933059Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def decode(self, input_ids, target_ids):\n    input_ids = input_ids.numpy().tolist()\n    target_ids = target_ids.numpy().tolist()\n    ar_texts = []\n    for j in range(len(input_ids)):\n        # Reconstruction initiale\n        letters = [self.letters[i] for i in input_ids[j]]\n        tashkeel = [self.tashkeel_list[i] for i in target_ids[j]]\n\n        # Filtrage des tokens spéciaux AVANT le zip\n        cleaned_pairs = []\n        for l, t in zip(letters, tashkeel):\n            if l not in ['<BOS>', '<EOS>', '<PAD>']:\n                cleaned_pairs.append((l, t))\n\n        # Combine letters + diacritics\n        bw_text = self.combine_tashkeel_with_text(cleaned_pairs)\n\n        # Translittération Buckwalter vers Arabe\n        try:\n            ar_text = transliterate_text(bw_text, 'bw2ar')\n        except:\n            ar_text = bw_text  # fallback in case transliteration fails\n\n        ar_texts.append(ar_text)\n\n    return ar_texts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:25:23.089248Z","iopub.execute_input":"2025-05-21T20:25:23.089517Z","iopub.status.idle":"2025-05-21T20:25:23.095239Z","shell.execute_reply.started":"2025-05-21T20:25:23.089498Z","shell.execute_reply":"2025-05-21T20:25:23.094698Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from keras.models import load_model\n\n# Charger le modèle\nmodel = load_model('/kaggle/working/best_model.keras', custom_objects={'CBHGModel': CBHGModel})\n\n# Texte non diacrité\ntext_input = \"الصبر مفتاح الفرج والعمل طريق التقدم والازدهار\"\n\n# Encodage\ninput_tensor, _ = tokenizer.encode(text_input, test_match=False)\ninput_tensor = tf.expand_dims(input_tensor, axis=0)\n\n# Prédiction\npred = model.predict(input_tensor)\npred_ids = tf.argmax(pred, axis=-1).numpy()[0]\n\n# Reconstruction Buckwalter\nspecial_tokens = ['<BOS>', '<EOS>', '<PAD>']\ndecoded_pairs = []\n\nfor letter_idx, tashkeel_idx in zip(input_tensor.numpy()[0], pred_ids):\n    if letter_idx >= len(tokenizer.letters):\n        continue\n    letter = tokenizer.letters[letter_idx]\n    if letter in special_tokens:\n        continue\n    tashkeel = tokenizer.tashkeel_list[tashkeel_idx] if tashkeel_idx < len(tokenizer.tashkeel_list) else ''\n    decoded_pairs.append((letter, tashkeel))\n\n# Reconstruction en Buckwalter\nreconstructed_bw = tokenizer.combine_tashkeel_with_text(decoded_pairs)\n\n# Translittération en arabe\n\nreconstructed_ar = transliterate_text(reconstructed_bw,direction='bw2ar')\n\n# Affichage\nprint(\"Texte original (sans voyelles) :\", text_input)\nprint(\"Texte reconstitué (avec voyelles - translittéré en arabe) :\", reconstructed_ar)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:39:30.271488Z","iopub.execute_input":"2025-05-21T20:39:30.272100Z","iopub.status.idle":"2025-05-21T20:39:32.385518Z","shell.execute_reply.started":"2025-05-21T20:39:30.272076Z","shell.execute_reply":"2025-05-21T20:39:32.384970Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\nTexte original (sans voyelles) : الصبر مفتاح الفرج والعمل طريق التقدم والازدهار\nTexte reconstitué (avec voyelles - translittéré en arabe) : الصَّبْرِ مِفْتَاحُ الْفَرَجِ وَالْعَمَلُ طَرِيقُ التَّقَدُّمِ وَالِازْدِهَارِ\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}